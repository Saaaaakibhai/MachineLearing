{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413b3a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in c:\\users\\hk305\\anaconda3\\lib\\site-packages (0.58.1)\n",
      "Collecting numba\n",
      "  Obtaining dependency information for numba from https://files.pythonhosted.org/packages/79/89/2d924ca60dbf949f18a6fec223a2445f5f428d9a5f97a6b29c2122319015/numba-0.60.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached numba-0.60.0-cp311-cp311-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba)\n",
      "  Obtaining dependency information for llvmlite<0.44,>=0.43.0dev0 from https://files.pythonhosted.org/packages/20/ab/ed5ed3688c6ba4f0b8d789da19fd8e30a9cf7fc5852effe311bc5aefe73e/llvmlite-0.43.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached llvmlite-0.43.0-cp311-cp311-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: numpy<2.1,>=1.22 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from numba) (1.24.3)\n",
      "Using cached numba-0.60.0-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "Using cached llvmlite-0.43.0-cp311-cp311-win_amd64.whl (28.1 MB)\n",
      "Installing collected packages: llvmlite, numba\n",
      "  Attempting uninstall: llvmlite\n",
      "    Found existing installation: llvmlite 0.41.1\n",
      "    Uninstalling llvmlite-0.41.1:\n",
      "      Successfully uninstalled llvmlite-0.41.1\n",
      "  Attempting uninstall: numba\n",
      "    Found existing installation: numba 0.58.1\n",
      "    Uninstalling numba-0.58.1:\n",
      "      Successfully uninstalled numba-0.58.1\n",
      "Successfully installed llvmlite-0.43.0 numba-0.60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ydata-profiling 4.6.4 requires numba<0.59.0,>=0.56.0, but you have numba 0.60.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: visions 0.7.5\n",
      "Uninstalling visions-0.7.5:\n",
      "  Successfully uninstalled visions-0.7.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping pandas-profiling as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ydata-profiling in c:\\users\\hk305\\anaconda3\\lib\\site-packages (4.6.4)\n",
      "Requirement already satisfied: scipy<1.12,>=1.4.1 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.11.1)\n",
      "Requirement already satisfied: pandas!=1.4.0,<3,>1.1 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (2.0.3)\n",
      "Requirement already satisfied: matplotlib<3.9,>=3.2 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (3.7.2)\n",
      "Requirement already satisfied: pydantic>=2 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (2.5.3)\n",
      "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (6.0)\n",
      "Requirement already satisfied: jinja2<3.2,>=2.11.1 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (3.1.2)\n",
      "Collecting visions[type_image_path]==0.7.5 (from ydata-profiling)\n",
      "  Obtaining dependency information for visions[type_image_path]==0.7.5 from https://files.pythonhosted.org/packages/62/fa/6a8539c83d2ccbd08d5f0c843b1784af9ff514e77f4c9d5d6800fdd340f6/visions-0.7.5-py3-none-any.whl.metadata\n",
      "  Using cached visions-0.7.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy<1.26,>=1.16.0 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.24.3)\n",
      "Requirement already satisfied: htmlmin==0.1.12 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.1.12)\n",
      "Requirement already satisfied: phik<0.13,>=0.11.1 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.12.4)\n",
      "Requirement already satisfied: requests<3,>=2.24.0 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (2.31.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.48.2 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (4.65.0)\n",
      "Requirement already satisfied: seaborn<0.13,>=0.10.1 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.12.2)\n",
      "Requirement already satisfied: multimethod<2,>=1.4 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.10)\n",
      "Requirement already satisfied: statsmodels<1,>=0.13.2 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.14.0)\n",
      "Requirement already satisfied: typeguard<5,>=4.1.2 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (4.1.5)\n",
      "Requirement already satisfied: imagehash==4.3.1 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (4.3.1)\n",
      "Requirement already satisfied: wordcloud>=1.9.1 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.9.3)\n",
      "Requirement already satisfied: dacite>=1.8 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.8.1)\n",
      "Collecting numba<0.59.0,>=0.56.0 (from ydata-profiling)\n",
      "  Obtaining dependency information for numba<0.59.0,>=0.56.0 from https://files.pythonhosted.org/packages/cd/59/5dd8a3059997ec1daf6f9f7c9b1aef7f0a9e23e1334a5774eae65cae6fd0/numba-0.58.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached numba-0.58.1-cp311-cp311-win_amd64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: PyWavelets in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from imagehash==4.3.1->ydata-profiling) (1.4.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from imagehash==4.3.1->ydata-profiling) (10.0.1)\n",
      "Requirement already satisfied: attrs>=19.3.0 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (22.1.0)\n",
      "Requirement already satisfied: networkx>=2.4 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (3.1)\n",
      "Requirement already satisfied: tangled-up-in-unicode>=0.0.4 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from jinja2<3.2,>=2.11.1->ydata-profiling) (2.1.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from matplotlib<3.9,>=3.2->ydata-profiling) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from matplotlib<3.9,>=3.2->ydata-profiling) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from matplotlib<3.9,>=3.2->ydata-profiling) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from matplotlib<3.9,>=3.2->ydata-profiling) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from matplotlib<3.9,>=3.2->ydata-profiling) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from matplotlib<3.9,>=3.2->ydata-profiling) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from matplotlib<3.9,>=3.2->ydata-profiling) (2.8.2)\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba<0.59.0,>=0.56.0->ydata-profiling)\n",
      "  Obtaining dependency information for llvmlite<0.42,>=0.41.0dev0 from https://files.pythonhosted.org/packages/14/fe/d3a9c921a2adad2e9f24693754983f290e0dac9410666e802b9dba4d0218/llvmlite-0.41.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached llvmlite-0.41.1-cp311-cp311-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from pandas!=1.4.0,<3,>1.1->ydata-profiling) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from pandas!=1.4.0,<3,>1.1->ydata-profiling) (2023.3)\n",
      "Requirement already satisfied: joblib>=0.14.1 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from phik<0.13,>=0.11.1->ydata-profiling) (1.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from pydantic>=2->ydata-profiling) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from pydantic>=2->ydata-profiling) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from pydantic>=2->ydata-profiling) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from requests<3,>=2.24.0->ydata-profiling) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from requests<3,>=2.24.0->ydata-profiling) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from requests<3,>=2.24.0->ydata-profiling) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from requests<3,>=2.24.0->ydata-profiling) (2023.11.17)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from statsmodels<1,>=0.13.2->ydata-profiling) (0.5.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from tqdm<5,>=4.48.2->ydata-profiling) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\hk305\\anaconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels<1,>=0.13.2->ydata-profiling) (1.16.0)\n",
      "Using cached numba-0.58.1-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "Using cached llvmlite-0.41.1-cp311-cp311-win_amd64.whl (28.1 MB)\n",
      "Using cached visions-0.7.5-py3-none-any.whl (102 kB)\n",
      "Installing collected packages: llvmlite, numba, visions\n",
      "  Attempting uninstall: llvmlite\n",
      "    Found existing installation: llvmlite 0.43.0\n",
      "    Uninstalling llvmlite-0.43.0:\n",
      "      Successfully uninstalled llvmlite-0.43.0\n",
      "  Attempting uninstall: numba\n",
      "    Found existing installation: numba 0.60.0\n",
      "    Uninstalling numba-0.60.0:\n",
      "      Successfully uninstalled numba-0.60.0\n",
      "Successfully installed llvmlite-0.41.1 numba-0.58.1 visions-0.7.5\n"
     ]
    }
   ],
   "source": [
    "#This Python3 environment comes with many helpful analytics libraries installed\n",
    "\n",
    "#It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "#For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import joblib\n",
    "!pip install -U numba\n",
    "!pip uninstall -y pandas-profiling visions\n",
    "!pip install ydata-profiling\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d78b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the packages\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from random import random\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,mean_absolute_error,classification_report,confusion_matrix\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "389fad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/combined_dataset.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f54012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 95910 entries, 0 to 95909\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   domain          95910 non-null  object\n",
      " 1   ranking         95910 non-null  int64 \n",
      " 2   isIp            95910 non-null  int64 \n",
      " 3   valid           95910 non-null  int64 \n",
      " 4   activeDuration  95910 non-null  int64 \n",
      " 5   urlLen          95910 non-null  int64 \n",
      " 6   is@             95910 non-null  int64 \n",
      " 7   isredirect      95910 non-null  int64 \n",
      " 8   haveDash        95910 non-null  int64 \n",
      " 9   domainLen       95910 non-null  int64 \n",
      " 10  nosOfSubdomain  95910 non-null  int64 \n",
      " 11  label           95910 non-null  int64 \n",
      "dtypes: int64(11), object(1)\n",
      "memory usage: 8.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62a97c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a08c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert th estring object to category\n",
    "for label,content in  df.items():\n",
    "    if pd.api.types.is_string_dtype(content):\n",
    "        df_tmp[label]=content.astype('category').cat.as_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a8d23f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 95910 entries, 0 to 95909\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   domain          95910 non-null  category\n",
      " 1   ranking         95910 non-null  int64   \n",
      " 2   isIp            95910 non-null  int64   \n",
      " 3   valid           95910 non-null  int64   \n",
      " 4   activeDuration  95910 non-null  int64   \n",
      " 5   urlLen          95910 non-null  int64   \n",
      " 6   is@             95910 non-null  int64   \n",
      " 7   isredirect      95910 non-null  int64   \n",
      " 8   haveDash        95910 non-null  int64   \n",
      " 9   domainLen       95910 non-null  int64   \n",
      " 10  nosOfSubdomain  95910 non-null  int64   \n",
      " 11  label           95910 non-null  int64   \n",
      "dtypes: category(1), int64(11)\n",
      "memory usage: 11.0 MB\n"
     ]
    }
   ],
   "source": [
    "# our dataset now contains no string type datatype\n",
    "df_tmp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64a6fcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert category to int data type\n",
    "for label,content in df_tmp.items():\n",
    "    if not pd.api.types.is_numeric_dtype(content):\n",
    "        df_tmp[label]=pd.Categorical(content).codes+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62508d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 95910 entries, 0 to 95909\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype\n",
      "---  ------          --------------  -----\n",
      " 0   domain          95910 non-null  int32\n",
      " 1   ranking         95910 non-null  int64\n",
      " 2   isIp            95910 non-null  int64\n",
      " 3   valid           95910 non-null  int64\n",
      " 4   activeDuration  95910 non-null  int64\n",
      " 5   urlLen          95910 non-null  int64\n",
      " 6   is@             95910 non-null  int64\n",
      " 7   isredirect      95910 non-null  int64\n",
      " 8   haveDash        95910 non-null  int64\n",
      " 9   domainLen       95910 non-null  int64\n",
      " 10  nosOfSubdomain  95910 non-null  int64\n",
      " 11  label           95910 non-null  int64\n",
      "dtypes: int32(1), int64(11)\n",
      "memory usage: 8.4 MB\n"
     ]
    }
   ],
   "source": [
    "#now all data is in int data types\n",
    "df_tmp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6651851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset in x ,and y.\n",
    "\n",
    "x=df_tmp.drop('label',axis=1)\n",
    "y=df_tmp['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "330f763f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set size: (76728, 11)\n",
      "Resampled training set size: (89462, 11)\n"
     ]
    }
   ],
   "source": [
    "# 1. Splitting the dataset into features (x) and target (y)\n",
    "x = df_tmp.drop('label', axis=1)  # Features (all columns except 'label')\n",
    "y = df_tmp['label']  # Target variable (label)\n",
    "\n",
    "# 2. Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 3. Scale features using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)  # Fit on train and transform train\n",
    "x_test_scaled = scaler.transform(x_test)  # Only transform test data\n",
    "\n",
    "# 4. Balance the dataset using SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "x_train_resampled, y_train_resampled = sm.fit_resample(x_train_scaled, y_train)\n",
    "\n",
    "# Check the new shape of resampled data\n",
    "print(f\"Original training set size: {x_train.shape}\")\n",
    "print(f\"Resampled training set size: {x_train_resampled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1019ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import the required libraries for models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# 2. Define the models dictionary\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20e8fe6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97      7999\n",
      "           1       0.97      0.98      0.98     11183\n",
      "\n",
      "    accuracy                           0.97     19182\n",
      "   macro avg       0.97      0.97      0.97     19182\n",
      "weighted avg       0.97      0.97      0.97     19182\n",
      "\n",
      "\n",
      "Model Performance Summary:\n",
      "['Random Forest', 0.9716922114482327, 0.9737310774710597, 0.9778234820710007, 0.9757729888903761]\n"
     ]
    }
   ],
   "source": [
    "# 1. Split the dataset into X and y (features and target) as you did before\n",
    "x = df_tmp.drop('label', axis=1)  # Features (all columns except 'label')\n",
    "y = df_tmp['label']  # Target variable (label)\n",
    "\n",
    "# 2. Split the data into training and testing sets (80% train, 20% test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 3. Scale features using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)  # Fit and scale training data\n",
    "x_test_scaled = scaler.transform(x_test)  # Scale test data using the same scaler\n",
    "\n",
    "# 4. Balance the dataset using SMOTE (only for the training data)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "x_train_resampled, y_train_resampled = sm.fit_resample(x_train_scaled, y_train)\n",
    "\n",
    "# 5. Model Training and Evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Initialize an empty list to store performance metrics\n",
    "model_performance = []\n",
    "\n",
    "# Iterate through the models\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(x_train_resampled, y_train_resampled)  # Train on resampled data\n",
    "    y_pred = model.predict(x_test_scaled)  # Predict on the scaled test data\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Print detailed classification report\n",
    "    print(f\"\\nClassification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Append performance metrics to the list\n",
    "    model_performance.append([name, accuracy, precision, recall, f1])\n",
    "\n",
    "# Print performance of all models\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "for performance in model_performance:\n",
    "    print(performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9dd3daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_jobs=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_jobs=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_jobs=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "model=RandomForestClassifier(n_jobs=1)\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e630b8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709623605463455"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "model_score=model.score(x_test,y_test)\n",
    "model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c9881be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   9.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   9.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   9.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   9.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  10.0s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=100, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=200; total time=   8.6s\n",
      "[CV] END max_depth=100, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=100, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=100, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=100, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=  21.8s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=  21.6s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=  21.7s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=  21.7s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=  21.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=10, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=10, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=10, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=10, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=10, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=10; total time=   0.1s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=10; total time=   0.1s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=10; total time=   0.1s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=10; total time=   0.1s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=10; total time=   0.1s\n",
      "\n",
      "=== RandomizedSearchCV Results ===\n",
      "Time taken: 265.9 seconds\n",
      "Best parameters: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
      "Best cross-validation score: 0.965\n",
      "Test set score: 0.9668\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 100, 200, 500],\n",
    "    'max_depth': [None, 5, 10, 50, 100],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize base model\n",
    "rf_model = RandomForestClassifier(random_state=42, n_jobs=1)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "print(\"Starting RandomizedSearchCV...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rs_rf = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_grid,\n",
    "    # n_iter=10,  # Uncomment to specify number of iterations\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rs_rf.fit(x_train, y_train)\n",
    "search_time = time.time() - start_time\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== RandomizedSearchCV Results ===\")\n",
    "print(\"Time taken:\", round(search_time, 2), \"seconds\")\n",
    "print(\"Best parameters:\", rs_rf.best_params_)\n",
    "print(\"Best cross-validation score:\", round(rs_rf.best_score_, 4))\n",
    "print(\"Test set score:\", round(rs_rf.score(x_test, y_test), 4))\n",
    "\n",
    "# Save best model\n",
    "best_model = rs_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d13fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
